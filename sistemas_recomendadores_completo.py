# -*- coding: utf-8 -*-
"""Sistemas Recomendadores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KZIvzY1UCBJTNckzwSSrpTUw6_2Xogy

#Sistemas Recomendadores

Pablo Díaz Acosta

Importamos módulos. Debemos instalar version antigua numpy para poder instalar surprise en Colab. Hay que ejecutar esta primera celda y luego reiniciar entorno (ctrl + m). Luego directamente ejecutamos la siguiente linea
"""

!pip uninstall -y numpy
!pip install numpy==1.26.4
!pip install scikit-surprise

from surprise import Dataset, Reader, KNNBaseline, SVD, accuracy
from surprise.model_selection import cross_validate, GridSearchCV
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error

"""Cargamos y preparamos el dataset principal"""

from google.colab import drive
drive.mount('/content/drive')

ruta = '/content/drive/MyDrive/u.data'

data = pd.read_csv(ruta, sep="\t", header=None)
data.columns = ['user_id', 'item_id', 'rating', 'timestamp']
data[['user_id', 'item_id']] = data[['user_id', 'item_id']].astype(str)

"""Información basica del dataset"""

n_usuarios = data['user_id'].nunique()
print("Número de usuarios distintos:", n_usuarios)

n_peliculas = data['item_id'].nunique()
print("Número de películas distintas:", n_peliculas)

n_valoraciones = len(data)
print("Número total de valoraciones:", n_valoraciones)

# Histograma ratings
data['rating'].value_counts().sort_index().plot(kind='bar')
plt.title("Distribución de valoraciones")
plt.xlabel("Rating")
plt.ylabel("Cantidad")
plt.grid(axis='y')
plt.show()

"""Preprocesado - Comprobaciones iniciales de inconsistencias en valoraciones

"""

print("Valores nulos:", data.isnull().sum())  # Nulos

# Valoraciones fuera de rango
ratings_fuera_de_rango = data[~data['rating'].between(1, 5)]
print("Valoraciones fuera del rango 1–5:", len(ratings_fuera_de_rango))

# Duplicados exactos
duplicados = data.duplicated().sum()
print("Filas duplicadas:", duplicados)

# Valoraciones repetidas por usuario y película
duplicados_user_item = data.duplicated(subset=['user_id', 'item_id'])
print("Valoraciones repetidas por usuario y película:", duplicados_user_item.sum())

"""Preprocesado - Carga y análisis de metadatos de películas (u.item)"""

ruta_item = '/content/drive/MyDrive/u.item'


peliculas = pd.read_csv(ruta_item, sep="|", encoding="latin-1", header=None)
peliculas.columns = [
    'item_id', 'titulo', 'fecha_estreno', 'fecha_video', 'URL',
    'unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy',
    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']

peliculas['item_id'] = peliculas['item_id'].astype(str)

"""Preprocesado - Normalización y detección de títulos duplicados"""

peliculas['titulo_normalizado'] = (
    peliculas['titulo']
    .str.lower()
    .str.strip()
    .str.replace(r'[^\w\s]', '', regex=True))

titulos_duplicados = peliculas.groupby('titulo_normalizado')['item_id'].nunique()
titulos_con_diferentes_ids = titulos_duplicados[titulos_duplicados > 1]

print("Títulos con múltiples item_id distintos (tras normalizar):", len(titulos_con_diferentes_ids))
print(titulos_con_diferentes_ids.head())

duplicados_reales = peliculas[peliculas['titulo_normalizado'].isin(titulos_con_diferentes_ids.index)]
duplicados_reales = duplicados_reales.sort_values('titulo')[['item_id', 'titulo']]
print("\nListado de títulos duplicados con sus item_id:")
print(duplicados_reales.head(10))

"""Eliminación de duplicados de películas según número de valoraciones. Borramos aquellos duplicados con menor número de votos"""

conteo_valoraciones = data['item_id'].value_counts().reset_index()
conteo_valoraciones.columns = ['item_id', 'num_valoraciones']

peliculas_con_val = peliculas.merge(conteo_valoraciones, on='item_id', how='left')
idx_max_val = peliculas_con_val.groupby('titulo_normalizado')['num_valoraciones'].idxmax()
peliculas_sin_duplicados = peliculas_con_val.loc[idx_max_val]

data = data[data['item_id'].isin(peliculas_sin_duplicados['item_id'])]

"""Comprobaciones adicionales de metadatos de películas

"""

data = data.merge(peliculas, on='item_id', how='left')

sin_fecha = data[data['fecha_estreno'].isnull()]
print("Películas distintas sin fecha de estreno:", sin_fecha['item_id'].nunique())
print("Títulos de películas sin fecha de estreno:")
print(sin_fecha['titulo'].dropna().unique())

peliculas_unknown = data[data['titulo'] == 'unknown']

print("Valores faltantes en u.item por columna:")
print(peliculas.isnull().sum())

# Películas con una sola valoración
valoraciones_por_pelicula = data.groupby('item_id').size()
print("Películas con solo una valoración:", sum(valoraciones_por_pelicula == 1))

# Títulos vacíos o 'unknown'
titulos_vacios = peliculas['titulo'].isin(['', 'unknown']).sum()
print("Películas con título vacío o 'unknown':", titulos_vacios)

# Fechas de estreno vacías
fechas_vacias = peliculas['fecha_estreno'].isin(['', ' ']).sum()
print("Fechas de estreno vacías (además de nulas):", fechas_vacias)

# URLs vacías
urls_vacias = peliculas['URL'].isin(['', ' ']).sum()
print("Películas con URL vacía:", urls_vacias)

# Películas sin géneros
columnas_generos = ['unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy',
                    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
sin_genero = peliculas[columnas_generos].sum(axis=1) == 0
print("Películas sin ningún género asignado:", sin_genero.sum())

"""Preprocesado - Comprobaciones sobre usuarios. Aunque readme nos asegura que todos los usuarios tienen al menos 20 valoraciones, comprobamos usuarios con 1 sola valoracion por si es el caso quitarlo del sistema"""

valoraciones_por_usuario = data.groupby('user_id').size()
print("Usuarios con solo una valoración:", sum(valoraciones_por_usuario == 1))

ids_en_data = set(data['item_id'].unique())
ids_en_peliculas = set(peliculas['item_id'].unique())
faltantes = ids_en_data - ids_en_peliculas
print("Item_id presentes en valoraciones pero no en u.item:", len(faltantes))

"""Carga y unión de metadatos de usuarios (u.user)"""

ruta_user = '/content/drive/MyDrive/u.user'
usuarios = pd.read_csv(ruta_user, sep="|", encoding="latin-1", header=None)
usuarios.columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
usuarios['user_id'] = usuarios['user_id'].astype(str)
data = data.merge(usuarios, on='user_id', how='left')

"""Comprobaciones de metadatos de usuarios"""

print("Valores nulos en columnas de usuario tras el merge:")
print(data[['age', 'gender', 'occupation', 'zip_code']].isnull().sum())
print("Géneros únicos:", data['gender'].unique())
print("Número de ocupaciones distintas:", data['occupation'].nunique())

"""Análisis de datos sospechosos en usuarios"""

ocupaciones_unicas = pd.DataFrame(sorted(data['occupation'].unique()), columns=['ocupacion'])
print("Lista de ocupaciones distintas:")
print(ocupaciones_unicas)

#Buscamos usuarios con edades menores a 5 y mayores a 100 ya que sería raro
usuarios_edades_sospechosas = data[(data['age'] < 5) | (data['age'] > 100)]
print("Número de usuarios con edades sospechosas:", usuarios_edades_sospechosas['user_id'].nunique())
print("Usuarios con edades fuera de rango razonable:")
print(usuarios_edades_sospechosas[['user_id', 'age', 'gender', 'occupation', 'zip_code']].drop_duplicates())

#Buscamos codigos postales erroneos
usuarios_codigo_sospechoso = data[data['zip_code'].isin(['', ' ', '00000'])]
print("Número de usuarios con código postal inválido:", usuarios_codigo_sospechoso['user_id'].nunique())
print("Usuarios con código postal vacío o inválido:")
print(usuarios_codigo_sospechoso[['user_id', 'age', 'gender', 'occupation', 'zip_code']].drop_duplicates())

#Numero de votos por pelicula
peliculas_con_votos = data.groupby('item_id').size().reset_index(name='num_votos').merge(
    data[['item_id', 'titulo']].drop_duplicates(), on='item_id').sort_values('num_votos', ascending=False)

"""Dividimos en train y test. Para ello definimos una funcion (split_no_cold) que divide el conjunto de datos en entrenamiento (data_train) y prueba (data_test) seleccionando un número fijo de valoraciones en test por usuario (n_test). La función asegura que todos los items presentes en el conjunto de prueba también están representado en entrenamiento, lo que evita problemas de cold start.

"""

#Dividimos en train y test

def split_no_cold(df, n_test=10, seed=42, verbose=True):
    rng = np.random.default_rng(seed)

    test_idx = (
        df.groupby('user_id', group_keys=False)
          .apply(lambda x: x.sample(n_test, random_state=rng))
          .index
    )

    test = df.loc[test_idx]
    train = df.drop(test_idx)

    orphan_items = set(test['item_id']) - set(train['item_id'])
    if orphan_items:
        mask = test['item_id'].isin(orphan_items)
        train = pd.concat([train, test[mask]])
        test  = test[~mask]
        if verbose:
            print(f"Movidos {len(orphan_items)} ítems huérfanos de test→train")

    if verbose:
        print(f"Train: {train.shape}, Test: {test.shape}")

    return train.reset_index(drop=True), test.reset_index(drop=True)

data_train, data_test = split_no_cold(data, n_test=10)

"""#Sistemas recomendadores

#Basado en filtro Colaborativo

##User-Based
"""

reader = Reader(rating_scale=(1, 5))  # Rango de valoraciones para surpruse
dataset_train = Dataset.load_from_df(data_train[['user_id', 'item_id', 'rating']], reader=reader) #Convertimos train al formato de surprise
trainset = dataset_train.build_full_trainset() #Objeto para poder trabajar correctamente con surprise

"""PASO 2: Ejecutamos GridSearchCV con validación cruzada (5-fold) para encontrar los mejores parametros que minimizan RMSE"""

param_grid = {
    'k': [5, 10, 15, 20, 25, 30, 35],
    'sim_options': {
        'name': ['cosine', 'pearson'],
        'user_based': [True] #Filtrado colaborativo basado en usuarios
    }
}
print("\nEvaluando modelo KNN (user-based) con GridSearchCV (RMSE y MAE)...\n")


gs = GridSearchCV(KNNBaseline, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=-1)
gs.fit(dataset_train)

# Mostramos los mejores parámetros encontrados
print("Mejores parámetros (RMSE):", gs.best_params['rmse'])
print("Mejor RMSE promedio:", gs.best_score['rmse'])
print("\nMejores parámetros (MAE):", gs.best_params['mae'])
print("Mejor MAE promedio:", gs.best_score['mae'])

df_resultados_knn = pd.DataFrame(gs.cv_results).sort_values(by='mean_test_rmse')
df_resultados_knn['k'] = df_resultados_knn['params'].apply(lambda d: d['k'])
df_resultados_knn['similitud'] = df_resultados_knn['params'].apply(lambda d: d['sim_options']['name'])

"""PASO 3: Visualizar resultados de GridSearchCV"""

print("\nTop 5 combinaciones simplificadas:\n")
print(df_resultados_knn[['k', 'similitud', 'mean_test_rmse', 'mean_test_mae']].head(5))

"""PASO 4: Entrenamiento del modelo final con TODO el dataset. Una vez encontrados mejores parametros entrenamos todo data train con los mismos."""

mejor_k = gs.best_params['rmse']['k']
mejor_sim = gs.best_params['rmse']['sim_options']['name']

mejor_modelo = KNNBaseline(k=mejor_k, sim_options={'name': mejor_sim, 'user_based': True}) #Creamos mejor modelo con los mejores hiperparámetros
trainset_final = dataset_train.build_full_trainset()  #  usamos data_train
#mejor_modelo.fit(trainset_final)

#Medimos tiempo de ejecución
import time

start_fit = time.time()
mejor_modelo.fit(trainset_final)
end_fit = time.time()
print(f"Tiempo de entrenamiento: {end_fit - start_fit}")
print(f"\nModelo final entrenado con: k = {mejor_k}, similitud = '{mejor_sim}', user_based = True\n")

"""PASO 5: Comparar real vs predicho. Evaluamos modelo final entrenado utilizando el conjunto data_test, que contiene valoraciones reales no vistas durante el entrenamiento"""

# Diccionario de títulos para sacar titulos en vez de ids.
diccionario_titulos = data.drop_duplicates('item_id').set_index('item_id')['titulo'].to_dict()

# Creamos testset explícito desde data_test
testset = list(zip(data_test['user_id'], data_test['item_id'], data_test['rating']))
#predicciones = mejor_modelo.test(testset)

#Medimos tiempo que tarda modelo en hacer predicciones
start_test = time.time()
predicciones = mejor_modelo.test(testset)
end_test = time.time()
print(f"Tiempo de predicción: {end_test - start_test}")


# Convertimos predicciones en DataFrame para análisis posterior
df_pred = pd.DataFrame({
    'user_id': [pred.uid for pred in predicciones],
    'item_id': [pred.iid for pred in predicciones],
    'real': [pred.r_ui for pred in predicciones],
    'predicha': [pred.est for pred in predicciones]
})

# Boxplot: real vs predicha
plt.figure(figsize=(8, 6))
sns.boxplot(x='real', y='predicha', data=df_pred)
plt.title("Distribución de valoraciones predichas según la real (data_test)")
plt.xlabel("Valoración real")
plt.ylabel("Valoración predicha")
plt.grid(True)
plt.show()

# Estadísticas por ítem (error absoluto) y mostramos un resumen estadístico

df_pred['error'] = abs(df_pred['real'] - df_pred['predicha'])
print("\nResumen de errores:")
print(df_pred['error'].describe())


# Agrupamos por items para analizar prediccion media vs valoración media
df_stats_items = df_pred.groupby('item_id').agg(
    num_valoraciones=('real', 'count'),
    media_valoracion=('real', 'mean'),
    media_predicha=('predicha', 'mean')
).reset_index()

#Calculamos la diferencia promedio entre valoracion real y predicha por item
df_stats_items['diferencia'] = abs(df_stats_items['media_valoracion'] - df_stats_items['media_predicha'])
df_stats_items['titulo'] = df_stats_items['item_id'].map(diccionario_titulos)

# Mejores y peores predicciones
mejor_predichas = df_stats_items.sort_values(by='diferencia').head(10).copy() #Menor error promedio
peor_predichas = df_stats_items.sort_values(by='diferencia', ascending=False).head(10).copy() #Mayor error promedio

print("\n Películas mejor predichas (predicción ≈ valoración real):\n")
print(mejor_predichas[['titulo', 'media_valoracion', 'media_predicha', 'diferencia', 'num_valoraciones']])

print("\n Películas peor predichas (mayor diferencia entre predicho y real):\n")
print(peor_predichas[['titulo', 'media_valoracion', 'media_predicha', 'diferencia', 'num_valoraciones']])

print(accuracy.rmse(predicciones))

#Pintamos distribucion de diferencias
plt.figure(figsize=(8, 5))
sns.histplot(df_stats_items['diferencia'], bins=30, kde=True)
plt.title("Distribución del error medio por película (valoración real vs predicha)")
plt.xlabel("Diferencia absoluta media")
plt.ylabel("Número de películas")
plt.grid(True)
plt.show()

""" PASO 6: Recomendaciones reales para un usuario específico. Identificamos que películas ya ha visto el usuario, obtenemos vecinos mas similares segun modelo. Para cada pelicula no vista comprobamos si ha sido valorada por alguno de sus vecinos. Si al menos 5 vecinos han valorado esa pelicula, predecimos la puntuacion que el modelo daría al usuario. Finalmente, ordenamos las peliculas no vistas por la prediccion del modelo y mostramos el top 10. Además, imprimimos número de vecinos que la han valorado y la media de los mismos para poder comparar"""

user_id = "123"  # Usuario para el que generamos recomendaciones

# Películas que ya ha visto el usuario (según data_train, que es lo que vio el modelo)
peliculas_todas = data['item_id'].unique()
pelis_vistas = data_train[data_train['user_id'] == user_id]['item_id'].values
pelis_no_vistas = [iid for iid in peliculas_todas if iid not in pelis_vistas]

# Obtener vecinos más similares desde el modelo entrenado
id_interno_usuario = trainset_final.to_inner_uid(user_id)
vecinos_internos = mejor_modelo.get_neighbors(id_interno_usuario, k=mejor_k)
vecinos_reales = [trainset_final.to_raw_uid(i) for i in vecinos_internos]

# Recolectar predicciones solo para películas que tengan valoraciones de los vecinos
datos_recomendaciones = []
for iid in pelis_no_vistas:
    valoraciones_vecinos = data_train[(data_train['user_id'].isin(vecinos_reales)) & (data_train['item_id'] == iid)]['rating']

    if not valoraciones_vecinos.empty:
        num_vecinos = valoraciones_vecinos.count()
        media_vecinos = valoraciones_vecinos.mean()
        titulo = diccionario_titulos.get(iid, 'Título no encontrado')

        if num_vecinos >= 5:  # Filtro mínimo como en el original
            prediccion = mejor_modelo.predict(user_id, iid).est

            datos_recomendaciones.append({
                'item_id': iid,
                'titulo': titulo,
                'media_vecinos': round(media_vecinos, 2),
                'num_vecinos': num_vecinos,
                'prediccion_modelo': round(prediccion, 2)
            })

# Creamos el DataFrame
df_recomendaciones_vecinos = pd.DataFrame(datos_recomendaciones)
df_recomendaciones_vecinos = df_recomendaciones_vecinos.sort_values(by='prediccion_modelo', ascending=False)

# Mostramos el top 10
top10_recomendaciones = df_recomendaciones_vecinos[['titulo', 'media_vecinos', 'prediccion_modelo', 'num_vecinos']].head(10)
print(f"\nTop 10 recomendaciones para el usuario {user_id} (según vecinos similares):\n")
print(top10_recomendaciones.to_string(index=False))

"""En esta siguiente sección generamos recomendaciones para el usuario 123 basadas únicamente en la predicción del modelo, sin aplicar ningún filtro sobre número mínimo de vecinos.  
Este enfoque permite obtener recomendaciones incluso para películas con pocas valoraciones, aunque puede perder precisión.

Después realizamos dos análisis complementarios:
1. Calculamos cuántos de los vecinos más similares han valorado cada película recomendada (top 10).
2. Mostramos la media de las valoraciones de esos vecinos sobre cada película, lo que ayuda a interpretar por qué se recomienda cada una.
"""

'''
# ------------------------------------------------------------------------------
# Recomendaciones sin filtro (ordenando solo por predicción del modelo)
# ------------------------------------------------------------------------------

user_id = 123

# Películas no vistas por el usuario en data_train (entrenamiento)
peliculas_todas = data['item_id'].unique()
pelis_vistas = data_train[data_train['user_id'] == user_id]['item_id'].values
pelis_no_vistas = [iid for iid in peliculas_todas if iid not in pelis_vistas]

# Predecimos para todas las no vistas
recomendaciones = []
for iid in pelis_no_vistas:
    pred = mejor_modelo.predict(user_id, iid)
    recomendaciones.append((iid, pred.est))

# Ordenamos por puntuación predicha
recomendaciones.sort(key=lambda x: x[1], reverse=True)

# Mostramos el top 10
print(f"\nTop 10 recomendaciones para el usuario {user_id} (sin filtro, por predicción):\n")
for iid, score in recomendaciones[:10]:
    titulo = diccionario_titulos.get(iid, 'Título no encontrado')
    print(f"{titulo} (ID: {iid}) → Predicción: {score:.2f}")


# ------------------------------------------------------------------------------
# PASO 7: Análisis de películas recomendadas según valoraciones de vecinos
# ------------------------------------------------------------------------------

# Reutilizamos el top 10 del paso anterior
top10 = [iid for iid, _ in recomendaciones[:10]]

# Recalculamos vecinos si fuera necesario (aunque ya está arriba)
id_interno_usuario = trainset_final.to_inner_uid(user_id)
vecinos_internos = mejor_modelo.get_neighbors(id_interno_usuario, k=mejor_k)
vecinos_reales = [trainset_final.to_raw_uid(i) for i in vecinos_internos]

# Analizamos las películas recomendadas
datos_agrupados = []
for iid in top10:
    titulo = diccionario_titulos.get(iid, "Título no encontrado")
    valoraciones_vecinos = data[(data['user_id'].isin(vecinos_reales)) & (data['item_id'] == iid)]['rating']

    if not valoraciones_vecinos.empty:
        datos_agrupados.append({
            'item_id': iid,
            'titulo': titulo,
            'media_vecinos': valoraciones_vecinos.mean(),
            'num_vecinos': valoraciones_vecinos.count()
        })
    else:
        datos_agrupados.append({
            'item_id': iid,
            'titulo': titulo,
            'media_vecinos': None,
            'num_vecinos': 0
        })

df_analisis_vecinos = pd.DataFrame(datos_agrupados)

print("\n Análisis de películas recomendadas según valoraciones de usuarios similares:\n")
print(df_analisis_vecinos[['titulo', 'media_vecinos', 'num_vecinos']])

# ------------------------------------------------------------------------------
# PASO 8: Comprobación manual de los vecinos que participaron en la predicción
# ------------------------------------------------------------------------------

# Reobtenemos los vecinos más similares
id_interno_usuario = trainset_final.to_inner_uid(user_id)
vecinos_internos_modelo = mejor_modelo.get_neighbors(id_interno_usuario, k=mejor_k)
vecinos_reales_modelo = [trainset_final.to_raw_uid(i) for i in vecinos_internos_modelo]

# Comprobamos cuántos de esos vecinos valoraron cada película recomendada (top 10)
for iid, pred_val in recomendaciones[:10]:
    vecinos_que_valoraron = data[(data['user_id'].isin(vecinos_reales_modelo)) & (data['item_id'] == iid)]['user_id'].tolist()
    num_usados_en_pred = len(vecinos_que_valoraron)
    titulo = diccionario_titulos.get(iid, 'Título no encontrado')

    print(f"{titulo}: {num_usados_en_pred} de los {mejor_k} vecinos más similares han valorado esta película")


'''

"""##Item Based

Ejecutamos GridSearchCV de nuevo con validación cruzada (5-fold) pero en este caso para Item-Based
"""

param_grid_item = {
    'k': [5, 10, 15, 20, 25, 30, 35],
    'sim_options': {
        'name': ['cosine', 'pearson'],
        'user_based': [False]  # Item-based
    }
}

print("\nEvaluando modelo KNN (item-based) con GridSearchCV (RMSE y MAE)...\n")

gs_item = GridSearchCV(KNNBaseline, param_grid_item, measures=['rmse', 'mae'], cv=5, n_jobs=-1)
gs_item.fit(dataset_train)  # ← usamos solo data_train

# Mostramos los mejores parámetros encontrados
print("Mejores parámetros (RMSE):", gs_item.best_params['rmse'])
print("Mejor RMSE promedio:", gs_item.best_score['rmse'])
print("\nMejores parámetros (MAE):", gs_item.best_params['mae'])
print("Mejor MAE promedio:", gs_item.best_score['mae'])

"""Visualizar resultados de GridSearchCV"""

# Visualización simplificada de resultados
df_resultados_item = pd.DataFrame(gs_item.cv_results).sort_values(by='mean_test_rmse')
df_resultados_item['k'] = df_resultados_item['params'].apply(lambda d: d['k'])
df_resultados_item['similitud'] = df_resultados_item['params'].apply(lambda d: d['sim_options']['name'])

print("\nTop 5 combinaciones simplificadas (item-based):\n")
print(df_resultados_item[['k', 'similitud', 'mean_test_rmse', 'mean_test_mae']].head(5))

""" Entrenamiento del modelo final con dataset de entrenamiento"""

mejor_k_item = gs_item.best_params['rmse']['k']
mejor_sim_item = gs_item.best_params['rmse']['sim_options']['name']

modelo_item = KNNBaseline(k=mejor_k_item, sim_options={'name': mejor_sim_item, 'user_based': False})
trainset_item = dataset_train.build_full_trainset()
#modelo_item.fit(trainset_item)

#Medimos tiempo
start_fit_item = time.time()
modelo_item.fit(trainset_item)
end_fit_item = time.time()
print(f" Tiempo de entrenamiento (item-based): {end_fit_item - start_fit_item:.2f} segundos")
print(f"\nModelo final entrenado con: k = {mejor_k_item}, similitud = '{mejor_sim_item}', user_based = False\n")

""" Comparar real vs predicho en todo el trainset"""

# Creamos el testset explícito desde data_test
testset_item = list(zip(data_test['user_id'], data_test['item_id'], data_test['rating']))
#predicciones_item = modelo_item.test(testset_item)

#Medimos tiempo
start_test_item = time.time()
predicciones_item = modelo_item.test(testset_item)
end_test_item = time.time()
print(f" Tiempo de predicción (item-based): {end_test_item - start_test_item:.2f} segundos")


# Creamos el DataFrame de predicciones
df_pred_item = pd.DataFrame({
    'user_id': [pred.uid for pred in predicciones_item],
    'item_id': [pred.iid for pred in predicciones_item],
    'real': [pred.r_ui for pred in predicciones_item],
    'predicha': [pred.est for pred in predicciones_item]
})

# Boxplot de valoraciones predichas vs reales
plt.figure(figsize=(8, 6))
sns.boxplot(x='real', y='predicha', data=df_pred_item)
plt.title("Distribución de valoraciones predichas según la real (item-based)")
plt.xlabel("Valoración real")
plt.ylabel("Valoración predicha")
plt.grid(True)
plt.show()

# Error absoluto por predicción
df_pred_item['error'] = abs(df_pred_item['real'] - df_pred_item['predicha'])
print("\nResumen de errores (item-based):")
print(df_pred_item['error'].describe())


# Agrupamos por ítem
df_stats_items_item = df_pred_item.groupby('item_id').agg(
    num_valoraciones=('real', 'count'),
    media_valoracion=('real', 'mean'),
    media_predicha=('predicha', 'mean')
).reset_index()

df_stats_items_item['diferencia'] = abs(df_stats_items_item['media_valoracion'] - df_stats_items_item['media_predicha'])
df_stats_items_item['titulo'] = df_stats_items_item['item_id'].map(diccionario_titulos)
print(accuracy.rmse(predicciones_item))

# Mejores y peores películas
mejor_predichas_item = df_stats_items_item.sort_values(by='diferencia').head(10).copy()
peor_predichas_item = df_stats_items_item.sort_values(by='diferencia', ascending=False).head(10).copy()

print("\n Películas mejor predichas (item-based):\n")
print(mejor_predichas_item[['titulo', 'media_valoracion', 'media_predicha', 'diferencia', 'num_valoraciones']])

print("\n Películas peor predichas (item-based):\n")
print(peor_predichas_item[['titulo', 'media_valoracion', 'media_predicha', 'diferencia', 'num_valoraciones']])

""" Recomendaciones reales para un usuario específico"""

user_id_item = "123"

# Películas no vistas por el usuario según data_train
peliculas_todas_item = data['item_id'].unique()
pelis_vistas_item = data_train[data_train['user_id'] == user_id_item]['item_id'].values
pelis_no_vistas_item = [iid for iid in peliculas_todas_item if iid not in pelis_vistas_item]

# Generamos recomendaciones basadas en similitud con ítems ya vistos
datos_recomendaciones_item = []

for iid in pelis_no_vistas_item:
    try:
        # Obtenemos ítems similares
        id_interno_item = trainset_item.to_inner_iid(iid)
        vecinos_items_internos = modelo_item.get_neighbors(id_interno_item, k=mejor_k_item)
        vecinos_items_reales = [trainset_item.to_raw_iid(i) for i in vecinos_items_internos]

        # Valoraciones del usuario sobre esas películas similares
        valoraciones_usuario = data_train[
            (data_train['user_id'] == user_id_item) &
            (data_train['item_id'].isin(vecinos_items_reales))
        ]['rating']

        if not valoraciones_usuario.empty:
            num_peliculas_similares = valoraciones_usuario.count()
            media_valoradas_similares = valoraciones_usuario.mean()
            titulo_item = diccionario_titulos.get(iid, 'Título no encontrado')

            if num_peliculas_similares >= 3:
                prediccion_item = modelo_item.predict(user_id_item, iid).est

                datos_recomendaciones_item.append({
                    'item_id': iid,
                    'titulo': titulo_item,
                    'media_vecinos': round(media_valoradas_similares, 2),
                    'num_peliculas_similares': num_peliculas_similares,
                    'prediccion_modelo': round(prediccion_item, 2)
                })

    except ValueError:
        continue

# Creamos DataFrame y mostramos top 10
df_recomendaciones_item = pd.DataFrame(datos_recomendaciones_item)
df_recomendaciones_item = df_recomendaciones_item.sort_values(by='prediccion_modelo', ascending=False)

top10_recomendaciones_item = df_recomendaciones_item[
    ['titulo', 'media_vecinos', 'prediccion_modelo', 'num_peliculas_similares']
].head(10)

print(f"\nTop 10 recomendaciones para el usuario {user_id_item} (item-based, ordenado por predicción):\n")
print(top10_recomendaciones_item.to_string(index=False))

"""Construmos diferentes funciones para poder evaluar precision@k recall@k y hitrate@k. Comenzamos con funcion generar_predicciones_colaborativo que genera predicciones para cada usuario del conjunto data_test, condisderando únicamente los items que no ha visto según data_train. Predice todas las valoraciones de los items no vistos y nos quedamos con topk para cada uno."""

from surprise import Prediction
from collections import defaultdict

def generar_predicciones_colaborativo(modelo, data_train, data_test, k=10):
    """
    Genera predicciones tipo Surprise.Prediction solo para ítems no vistos.
    """
    all_items = data_train['item_id'].unique()
    usuarios = data_test['user_id'].unique()

    recomendaciones = []
    for uid in usuarios:
        vistos = set(data_train[data_train['user_id'] == uid]['item_id'])
        no_vistos = [iid for iid in all_items if iid not in vistos]

        # Predecimos para ítems no vistos
        preds = [modelo.predict(uid, iid) for iid in no_vistos]
        topk = sorted(preds, key=lambda x: x.est, reverse=True)[:k]

        recomendaciones.extend(topk)

    return recomendaciones

predicciones_user_reales = generar_predicciones_colaborativo(mejor_modelo, data_train, data_test, k=10)
predicciones_item_reales = generar_predicciones_colaborativo(modelo_item, data_train, data_test, k=10)

"""Creamos funcion precicion@k. Con esta función, evaluamos la calidad de las recomendaciones generadas por los modelos.  Esta métrica mide la proporción de ítems relevantes (valoración ≥ 3.5 en data_test) que aparecen entre los K primeros ítems recomendados para cada usuario.

Lo que hacemos es agrupar las predicciones por usuario y quedarnos con las k mas altas. Comparamos con las valoraciones reales del conjunto data_test. Luego, calculamos la fracción de items relevantes correctamente recomendados. Finalmente, promediamos la precision entre todos los usuarios.
"""

def precision_at_k(predicciones, data_test, k=10, threshold=3.5):
    """
    Calcula Precision@K para un conjunto de predicciones.
    - predicciones: lista de objetos Prediction
    - data_test: DataFrame con las valoraciones reales
    """
    # Agrupamos todas las predicciones por usuario
    top_predichas = defaultdict(list)
    for pred in predicciones:
        top_predichas[pred.uid].append((pred.iid, pred.est))

    # Ordenamos y seleccionamos top-K por usuario
    for uid in top_predichas:
        top_predichas[uid] = sorted(top_predichas[uid], key=lambda x: x[1], reverse=True)[:k]

    # Convertimos data_test en un set de (user_id, item_id) relevantes
    relevantes_reales = set(
        data_test[data_test['rating'] >= threshold][['user_id', 'item_id']]
        .itertuples(index=False, name=None)
    )

    # Calculamos Precision@K por usuario
    precisiones = []
    for uid in top_predichas:
        predichos = set((uid, iid) for iid, _ in top_predichas[uid])
        verdaderos = set(filter(lambda x: x[0] == uid, relevantes_reales))

        if len(predichos) > 0:
            precision = len(predichos & verdaderos) / len(predichos)
            precisiones.append(precision)

    return sum(precisiones) / len(precisiones) if precisiones else 0
#Calculamos precision@10 para modelo user-based
precision_user = precision_at_k(predicciones_user_reales, data_test, k=10, threshold=3.5)
print(f"Precision@10 (user-based): {precision_user:.4f}")

"""Creamos funcion recall@k. Esta métrica mide qué proporción de los ítems relevantes (valoraciones reales ≥ 3.5) fueron incluidos entre las `K recomendaciones que el sistema generó para cada usuario. Para cada usuario, extraemos los items que realmente le gustaron de data_test. Comparamos con las k recomendaciones generadas por el modelo. Calculamos el porcentaje de relevantes que fueron acertados y promediamos entre todos los usuarios."""

from collections import defaultdict

def recall_at_k(predicciones, data_test, k=10, threshold=3.5):
    """
    Calcula Recall@K para un conjunto de predicciones.
    - predicciones: lista de objetos Prediction (de Surprise)
    - data_test: DataFrame con valoraciones reales
    """
    # Agrupamos predicciones por usuario
    top_predichas = defaultdict(list)
    for pred in predicciones:
        top_predichas[pred.uid].append((pred.iid, pred.est))

    # Ordenamos y seleccionar top-K
    for uid in top_predichas:
        top_predichas[uid] = sorted(top_predichas[uid], key=lambda x: x[1], reverse=True)[:k]

    # Diccionario de ítems relevantes reales por usuario (según data_test)
    relevantes_reales = defaultdict(set)
    for row in data_test.itertuples(index=False):
        if row.rating >= threshold:
            relevantes_reales[row.user_id].add(row.item_id)

    # Calculamos recall@k por usuario
    recalls = []
    for uid in top_predichas:
        predichos = set(iid for iid, _ in top_predichas[uid])
        verdaderos = relevantes_reales.get(uid, set())

        if verdaderos:
            recall = len(predichos & verdaderos) / len(verdaderos)
            recalls.append(recall)

    return sum(recalls) / len(recalls) if recalls else 0

#Calculamos para user-based
recall_user = recall_at_k(predicciones_user_reales, data_test, k=10, threshold=3.5)
print(f"Recall@10 (user-based): {recall_user:.4f}")

"""Funcion hitrate@k. Evalua si al menos unas de las k recomendaciones generadas para cada usuario ha sido relevante, es decir, ha coincidido con alguna valoracion alta (>=3.5). Para cada usuario seleccionamos sus k recomendaciones, comprobamos si alguna de ellas esta entre sus items relevantes reales, contamos un hit si hay al menos una coincidencia. Finalmente, calculamos porcenataje de usuarios con al menos un hit"""

def hit_rate_at_k(predicciones, data_test, k=10, threshold=3.5):
    hits = 0
    total = 0
    pred_por_usuario = defaultdict(list)
    #Agrupamos las predicciones por usuarios
    for pred in predicciones:
        pred_por_usuario[pred.uid].append((pred.iid, pred.est))
    #Evaluamos por usuario
    for uid in pred_por_usuario:
        top_k = sorted(pred_por_usuario[uid], key=lambda x: x[1], reverse=True)[:k]
        reales = set(data_test[(data_test['user_id'] == uid) & (data_test['rating'] >= threshold)]['item_id'])

        if any(iid in reales for iid, _ in top_k): #Si al menos uno de los top-k está en relevantes sumamos hit
            hits += 1
        total += 1

    return hits / total if total > 0 else 0 #Porcentaje de usuarios con al menos una recomendacion relevante
#Calculamos hit rate para user-based
hitrate_user = hit_rate_at_k(predicciones_user_reales, data_test, k=10, threshold=3.5)
print(f"Hit Rate@10 (user-based): {hitrate_user:.4f}")

"""Valores para item based"""

precision_item = precision_at_k(predicciones_item_reales, data_test, k=10, threshold=3.5)
recall_item = recall_at_k(predicciones_item_reales, data_test, k=10, threshold=3.5)
hitrate_item = hit_rate_at_k(predicciones_item_reales, data_test, k=10, threshold=3.5)

print(f"Precision@10 (item-based): {precision_item:.4f}")
print(f"Recall@10    (item-based): {recall_item:.4f}")
print(f"Hit Rate@10  (item-based): {hitrate_item:.4f}")

"""#Basados en Contenido

En este bloque construimos una representación numérica del contenido de las películas basada en sus géneros.
"""

#------------------------------------------------------------------------------
#Basado en contenido
#------------------------------------------------------------------------------

# Aseguramos que el índice esté limpio y alineado
peliculas_sin_duplicados = peliculas_sin_duplicados.reset_index(drop=True)

# Creamos una columna nueva con los géneros activos como texto
peliculas_sin_duplicados['generos_texto'] = peliculas_sin_duplicados[columnas_generos] \
    .apply(lambda fila: ' '.join([genero for genero, activo in zip(columnas_generos, fila) if activo == 1]), axis=1)

# Creamos el vectorizador
vectorizador = TfidfVectorizer()

# Aplicamos TF-IDF sobre la columna de texto con los géneros
matriz_tfidf = vectorizador.fit_transform(peliculas_sin_duplicados['generos_texto'])

# Calculamos la similitud coseno entre todas las películas
similitud_peliculas = cosine_similarity(matriz_tfidf)

"""Definimos funcion que permite recomendar peliculas similares a una dada usando únicamente el contenido (géneros) como criterio. Para ello, localizamos índice de película cuyo titulo coincide exactamente con nombre dado, obtenemos su vector de similitd con el resto de peliculas, filtramos peliculas con al menos 10 votos (para asegurar fiabilidad), ordenamos por similitud (y numero de votos en caso de empate). Monstramos el top de peliculas similares, sus generes y su puntuacion de similitud."""

#Recomendamos similares

def recomendar_similares_titulo_exacto(nombre_exacto, n=10):
    """
    Recomienda películas similares solo si el título coincide exactamente con 'nombre_exacto'.
    Muestra título, similitud, número de votos y los géneros de la película base y recomendadas.
    """
    coincidencias = peliculas_sin_duplicados[peliculas_sin_duplicados['titulo'] == nombre_exacto]

    if coincidencias.empty:
        print(f"No se encontró ninguna película con el título exacto: '{nombre_exacto}'")
        return

    idx = coincidencias.index[0]
    similitudes = similitud_peliculas[idx]

    # Obtener géneros de la película base
    generos_base = peliculas_sin_duplicados.loc[idx, 'generos_texto']
    print(f"\nPelícula base: {peliculas_sin_duplicados.loc[idx, 'titulo']}")
    print(f"Géneros: {generos_base}\n")

    # Filtrar películas con al menos 10 votos
    candidatos = []
    for i in range(len(peliculas_sin_duplicados)):
        if i == idx:
            continue
        votos = peliculas_sin_duplicados.iloc[i]['num_valoraciones']
        if pd.notnull(votos) and votos >= 10:
            candidatos.append((i, similitudes[i], votos))

    # Ordenar por similitud y votos
    candidatos_ordenados = sorted(candidatos, key=lambda x: (-x[1], -x[2]))
    recomendaciones = candidatos_ordenados[:n]

    print(f"Películas similares (al menos 10 votos):\n")
    for i, sim, votos in recomendaciones:
        fila = peliculas_sin_duplicados.iloc[i]
        titulo = fila['titulo']
        generos = fila['generos_texto']
        print(f"{titulo}  (similitud: {sim:.3f}, votos: {votos})")
        print(f"  → Géneros: {generos}\n")

recomendar_similares_titulo_exacto("Alien (1979)", n=10)

"""Esta funcion genera recomendaciones para un usuario específico basandonse en contenido. Seleccionamos las peliculas que usario ha valorado positivamente >=4 en data_train. Localizamos los vectores de esas peliculas en la matrizs de similitud. Suma las filas correspondeintes para obtener una similitud acumulada entre todas las peliculas del dataset y las favoritas del usuario. Filtra peliculas que no haya ya visto el usuario y que tengna al menos 10 votos. Ordenamos las candidatas por similitud acumulada y número de votos, mostrandolas."""

#Recomendar pelicula para usuario

def recomendar_para_usuario(user_id, n=10, min_votos=10):
    """
    Recomienda películas a un usuario basándose en las que valoró positivamente en data_train.
    Muestra similitud acumulada, géneros y número de votos.

    Parámetros:
    - user_id: ID del usuario
    - n: número de recomendaciones (por defecto 10)
    - min_votos: mínimo de votos para considerar una película (por defecto 10)
    """
    favoritas = data_train[(data_train['user_id'] == user_id) & (data_train['rating'] >= 4)]

    if favoritas.empty:
        print(f"El usuario {user_id} no tiene valoraciones positivas suficientes en data_train.")
        return

    ids_vistas = set(favoritas['item_id'])
    indices_vistas = peliculas_sin_duplicados[
        peliculas_sin_duplicados['item_id'].isin(ids_vistas)
    ].index

    sim_acumulada = similitud_peliculas[indices_vistas].sum(axis=0)

    recomendaciones = []
    for i in range(len(peliculas_sin_duplicados)):
        item_id = peliculas_sin_duplicados.iloc[i]['item_id']
        votos = peliculas_sin_duplicados.iloc[i]['num_valoraciones']
        if (
            item_id not in ids_vistas and
            pd.notnull(votos) and
            votos >= min_votos and
            sim_acumulada[i] > 0
        ):
            recomendaciones.append((i, sim_acumulada[i], votos))

    recomendaciones_ordenadas = sorted(recomendaciones, key=lambda x: (-x[1], -x[2]))[:n]

    print(f"\nRecomendaciones para el usuario {user_id} (basado en contenido):\n")
    for i, sim, votos in recomendaciones_ordenadas:
        fila = peliculas_sin_duplicados.iloc[i]
        titulo = fila['titulo']
        generos = fila['generos_texto']
        print(f"{titulo}  (similitud acumulada: {sim:.3f}, votos: {int(votos)})")
        print(f"  → Géneros: {generos}\n")

recomendar_para_usuario("123", n=10)

'''
#------------------------------------------------------------------------------
#Comprobacion
#------------------------------------------------------------------------------

# 1. Eliminamos posibles columnas duplicadas por fusiones anteriores
data = data.drop(columns=['titulo'], errors='ignore')

# 2. Extraemos las valoraciones positivas del usuario
favoritas_usuario = data[(data['user_id'] == 7) & (data['rating'] >= 5)]

# 3. Añadimos los títulos y géneros de esas películas desde peliculas_sin_duplicados
gustos_usuario = favoritas_usuario.merge(
    peliculas_sin_duplicados[['item_id', 'titulo', 'generos_texto']],
    on='item_id',
    how='left'
)

# 4. Mostramos las películas y sus géneros
print(gustos_usuario[['titulo', 'generos_texto']].head(10))
'''

"""Vamos ahora a crear las funciones necesarias para calcular precision@k, recall@k y hitrate@k y asi poder comparar con filtrado colaborativo. Es necesario crear estas funciones ya que colaborativo lo hicimos con surprise y no es compatible. Empezamos con funcion recomendar_para_usuario_evaluacion. Para cada usuario, identificamos peliculas que valoro positviamente con >=3.5 en data_train. A partir de esas favoritas, sumamos similitudes de contenido con resto del catalogo, seleccionaos las mas similares que el usuario aun no ha visto y guardamos como tupla para futuro."""

def recomendar_para_usuario_evaluacion(user_id, n=10):
    """
    devuelve lista (user_id, item_id, score)
    """
    favoritas = data_train[
        (data_train['user_id'] == user_id) &
        (data_train['rating'] >= 3.5)
    ]

    if favoritas.empty:
        return []

    ids_vistas = set(favoritas['item_id'])
    indices_vistas = peliculas_sin_duplicados[
        peliculas_sin_duplicados['item_id'].isin(ids_vistas)
    ].index

    sim_acumulada = similitud_peliculas[indices_vistas].sum(axis=0)

    recomendaciones = []
    for i in range(len(peliculas_sin_duplicados)):
        item_id = peliculas_sin_duplicados.iloc[i]['item_id']
        # Eliminamos cualquier chequeo de 'votos'
        if item_id not in ids_vistas and sim_acumulada[i] > 0:
            recomendaciones.append((item_id, sim_acumulada[i]))

    # Ordenar por similitud acumulada y quedarnos con n
    recomendaciones_ordenadas = sorted(
        recomendaciones, key=lambda x: -x[1]
    )[:n]

    return [(user_id, item_id, sim_score)
            for item_id, sim_score in recomendaciones_ordenadas]



usuarios_test = data_test['user_id'].unique()
predicciones_cb = []

for uid in usuarios_test:
    predicciones_cb.extend(recomendar_para_usuario_evaluacion(uid, n=10))

"""Calculamos precision@k basado en contenido."""

#------------------------------------------------------------------------------
#Precision@k
#------------------------------------------------------------------------------
def precision_at_k_contenido(predicciones, data_test, k=10):

    from collections import defaultdict

    top_predichas = defaultdict(list)
    for uid, iid, score in predicciones:
        top_predichas[uid].append((iid, score))

    # Ordenamos por score y dejamos top K por usuario
    for uid in top_predichas:
        top_predichas[uid] = sorted(top_predichas[uid], key=lambda x: x[1], reverse=True)[:k]

    # Relevantes reales en test (rating ≥ 3.5)
    relevantes_reales = set(
        data_test[data_test['rating'] >= 3.5][['user_id', 'item_id']]
        .itertuples(index=False, name=None)
    )

    # Precision@K por usuario
    precisiones = []
    for uid in top_predichas:
        predichos = set((uid, iid) for iid, _ in top_predichas[uid])
        verdaderos = set(filter(lambda x: x[0] == uid, relevantes_reales))

        if len(predichos) > 0:
            precision = len(predichos & verdaderos) / len(predichos)
            precisiones.append(precision)

    return sum(precisiones) / len(precisiones) if precisiones else 0


precision_cb = precision_at_k_contenido(predicciones_cb, data_test, k=10)
print(f"Precision@10 (content-based): {precision_cb:.4f}")

"""Continuamos ahora con recall@k"""

#Recall@k

def recall_at_k_contenido(predicciones, data_test, k=10):
    """
    Versión específica para modelo basado en contenido:
    - predicciones: lista de (user_id, item_id, sim_score)
    - NO filtra por score
    - Calcula Recall@K
    """
    from collections import defaultdict

    # Agrupamos predicciones por usuario
    top_predichas = defaultdict(list)
    for uid, iid, score in predicciones:
        top_predichas[uid].append((iid, score))

    # Ordenamos y dejamos top K
    for uid in top_predichas:
        top_predichas[uid] = sorted(top_predichas[uid], key=lambda x: x[1], reverse=True)[:k]

    # Construimos conjunto de relevantes en data_test (rating >= 4)
    relevantes_reales = defaultdict(set)
    for row in data_test.itertuples(index=False):
        if row.rating >= 3.5:
            relevantes_reales[row.user_id].add(row.item_id)

    # Calculamos Recall@K por usuario
    recalls = []
    for uid in top_predichas:
        predichos = set(iid for iid, _ in top_predichas[uid])
        verdaderos = relevantes_reales.get(uid, set())

        if verdaderos:
            recall = len(predichos & verdaderos) / len(verdaderos)
            recalls.append(recall)

    return sum(recalls) / len(recalls) if recalls else 0

recall_cb = recall_at_k_contenido(predicciones_cb, data_test, k=10)
print(f"Recall@10 (content-based): {recall_cb:.4f}")

"""Y por último calculamos hitrate@k"""

def hit_rate_at_k_contenido(predicciones, data_test, k=10):
    from collections import defaultdict
    pred_por_usuario = defaultdict(list)

    for uid, iid, score in predicciones:
        pred_por_usuario[uid].append((iid, score))

    for uid in pred_por_usuario:
        pred_por_usuario[uid] = sorted(pred_por_usuario[uid], key=lambda x: x[1], reverse=True)[:k]

    hits = 0
    total = 0

    for uid in pred_por_usuario:
        reales = set(data_test[(data_test['user_id'] == uid) & (data_test['rating'] >= 3.5)]['item_id'])
        predichos = set(iid for iid, _ in pred_por_usuario[uid])
        if predichos & reales:
            hits += 1
        total += 1

    return hits / total if total > 0 else 0


hitrate_cb = hit_rate_at_k_contenido(predicciones_cb, data_test, k=10)
print(f"Hit Rate@10 (content-based): {hitrate_cb:.4f}")

"""#Sistema basado en Deep Learning con Embeddings

En este bloque preparamos los datos para entrenar una red neuronal que utiliza capas de embeddings para representar usuarios e ítems.
"""

#------------------------------------------------------------------------------
#Red neuronal con embeddings
#------------------------------------------------------------------------------
import os
import random
import tensorflow as tf

#Establecemos semilla para reproducibilidad
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)


#Preparamos los datos para poder aplicar red neuronal Convertimos id reales en indices ordenados para poder usar

# Creamos diccionarios de mapeo
user2idx = {user_id: idx for idx, user_id in enumerate(data_train['user_id'].unique())}
item2idx = {item_id: idx for idx, item_id in enumerate(data_train['item_id'].unique())}
idx2item = {v: k for k, v in item2idx.items()}

# Aplicamos los mapeos en data train
data_train['user_idx'] = data_train['user_id'].map(user2idx)
data_train['item_idx'] = data_train['item_id'].map(item2idx)
#Mapeos en data test
data_test['user_idx'] = data_test['user_id'].map(user2idx)
data_test['item_idx'] = data_test['item_id'].map(item2idx)


# Eliminamos de test las filas con usuarios o ítems no vistos en train
data_test = data_test.dropna(subset=['user_idx', 'item_idx'])
data_test = data_test.astype({'user_idx': int, 'item_idx': int})


# Creamos matrices de entrada y salida desde data_train y data_test
X_train = data_train[['user_idx', 'item_idx']].values
y_train = data_train['rating'].values

X_test = data_test[['user_idx', 'item_idx']].values
y_test = data_test['rating'].values
#Mostramos tamaño de los conjuntos
print("Tamaño del conjunto de entrenamiento:", len(X_train))
print("Tamaño del conjunto de test:", len(X_test))

"""Definimos el modelo de red neuronal con embeddings y compilamos. Cada usuario y pelicula tiene su propio embedding. El modelo aprende a predecir valoracion que un usuario le dara a una pelicula basandose en el vector de usuario y vector de esa pelicula."""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# Número de usuarios y de películas
n_users = data_train['user_idx'].nunique()
n_items = data_train['item_idx'].nunique()
embedding_size = 50  # Tamaño de los vectores de embeddings

# Entradas
input_user = Input(shape=(1,), name='usuario')
input_item = Input(shape=(1,), name='pelicula')

# Embeddings
embedding_user = Embedding(input_dim=n_users, output_dim=embedding_size)(input_user)
embedding_item = Embedding(input_dim=n_items, output_dim=embedding_size)(input_item)

# Aplanamos
user_vec = Flatten()(embedding_user)
item_vec = Flatten()(embedding_item)

# Concatenamos y conectamos capas
concatenado = Concatenate()([user_vec, item_vec])
densa_1 = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(concatenado)
salida = Dense(1, activation='linear')(densa_1)

# Modelo final
modelo = Model(inputs=[input_user, input_item], outputs=salida)

# Compilamos
modelo.compile(
    loss='mean_squared_error',
    optimizer=Adam(learning_rate=0.001),
    metrics=['RootMeanSquaredError']
)

# Mostramos resumen
modelo.summary()

"""En este bloque entrenamos el modelo definido anteriormente y evaluamos su rendimiento básico con RMSE sobre el conjunto de test."""

# Separar entradas para Keras
user_train = X_train[:, 0]
item_train = X_train[:, 1]

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)
import time
start_fit_nn = time.time()

# Entrenamiento
historial = modelo.fit(
    [user_train, item_train],
    y_train,
    batch_size=64,
    epochs=10,
    validation_split=0.2,
    verbose=1,
    callbacks=[early_stop]
)


end_fit_nn = time.time()
print(f" Tiempo de entrenamiento (red neuronal): {end_fit_nn - start_fit_nn:.2f} segundos")

from sklearn.metrics import mean_squared_error

#Separamos test para Keras
user_test = X_test[:, 0]
item_test = X_test[:, 1]

start_pred_nn = time.time()
#Generamos predicciones para conjunto de test
y_pred = modelo.predict([user_test, item_test], verbose=0)
y_pred = np.clip(y_pred, 1.0, 5.0) #Limitamos rango entre 1 y 5

end_pred_nn = time.time()

# RMSE
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
print(f" RMSE en el conjunto de test (red neuronal): {rmse_test:.4f}")
print(f" Tiempo de predicción (red neuronal): {end_pred_nn - start_pred_nn:.2f} segundos")

"""Representamos reales vs predichas"""

#Representacion

# Creamos un DataFrame con los resultados
df_resultados = pd.DataFrame({
    'real': y_test,
    'predicha': y_pred.flatten()
})

# Boxplot de predicho vs real
plt.figure(figsize=(8, 6))
sns.boxplot(x='real', y='predicha', data=df_resultados)
plt.title("Distribución de valoraciones predichas según la real (Red Neuronal)")
plt.xlabel("Valoración real")
plt.ylabel("Valoración predicha")
plt.grid(True)
plt.show()

# Cálculo del error absoluto y resumen estadístico
df_resultados['error'] = abs(df_resultados['real'] - df_resultados['predicha'])
print("\nResumen de errores (Red Neuronal):")
print(df_resultados['error'].describe())

"""Mejores y peores películas predichas. 1. Creamos un DataFrame con cada predicción en el conjunto de test (usuario, película, real, predicho).
2. Agrupamos las predicciones por película (item_id) y calculamos:
   - Número total de valoraciones (num_valoraciones)
   - Media de valoraciones reales (media_real)
   - Media de valoraciones predichas (media_predicha)
3. Se calcula la diferencia absoluta entre ambas medias.
4. Se extraen:
   - Las 10 películas mejor predichas (menor diferencia)
   - Las 10 peor predichas (mayor diferencia)
"""

#Mejores y peores
# Resultados por ítem
df_eval = pd.DataFrame({
    'user_idx': X_test[:, 0],
    'item_idx': X_test[:, 1],
    'rating_real': y_test,
    'rating_pred': y_pred.flatten()
})

# Mapeamos item_idx → item_id → título
df_eval['item_id'] = df_eval['item_idx'].map(idx2item)
df_eval = df_eval.merge(data_test[['item_id', 'titulo']].drop_duplicates(), on='item_id', how='left')

# Agrupamos por película
df_stats = df_eval.groupby('item_id').agg(
    num_valoraciones=('rating_real', 'count'),
    media_real=('rating_real', 'mean'),
    media_predicha=('rating_pred', 'mean'),
    titulo=('titulo', 'first')
).reset_index()

# Calculamos la diferencia absoluta
df_stats['diferencia'] = abs(df_stats['media_real'] - df_stats['media_predicha'])

# Top 10 mejor predichas
mejor_predichas = df_stats.sort_values(by='diferencia').head(10)

# Top 10 peor predichas
peor_predichas = df_stats.sort_values(by='diferencia', ascending=False).head(10)

# Mostrar resultados
print("\n Películas mejor predichas (predicho ≈ real):\n")
print(mejor_predichas[['titulo', 'media_real', 'media_predicha', 'diferencia', 'num_valoraciones']])

print("\n Películas peor predichas (gran diferencia):\n")
print(peor_predichas[['titulo', 'media_real', 'media_predicha', 'diferencia', 'num_valoraciones']])

"""Recomendación para usuario concreto. Identificamos peliculas no vistas segun data train, construimos entradas de prediccion para todas las peliculas no vistas por el usuario. Predecimos las valoraciones, ordenamos por puntuaciones predichas y elegimos top 10."""

def recomendar_peliculas_usuario(user_id_original, data, modelo, user2idx, item2idx, idx2item, top_n=10):
    if user_id_original not in user2idx:
        print("El usuario no existe en el dataset.")
        return pd.DataFrame()

    user_idx = user2idx[user_id_original]
    peliculas_vistas = set(data_train[data_train['user_id'] == user_id_original]['item_id'])

    todas_peliculas = set(item2idx.keys())
    peliculas_no_vistas = list(todas_peliculas - peliculas_vistas)

    item_idx_no_vistas = [item2idx[i] for i in peliculas_no_vistas if i in item2idx]
    user_idx_array = [user_idx] * len(item_idx_no_vistas)

    if not item_idx_no_vistas:
        print("No hay películas no vistas por el usuario que se puedan predecir.")
        return pd.DataFrame()

    predicciones = modelo.predict([np.array(user_idx_array), np.array(item_idx_no_vistas)], verbose=0)
    predicciones = np.clip(predicciones, 1.0, 5.0)

    recomendaciones = pd.DataFrame({
        'item_idx': item_idx_no_vistas,
        'predicho': predicciones.flatten()
    })
    recomendaciones['item_id'] = recomendaciones['item_idx'].map(idx2item)

    # Añadir título
    titulos_unicos = data[['item_id', 'titulo']].drop_duplicates()
    recomendaciones = recomendaciones.merge(titulos_unicos, on='item_id', how='left')

    # Añadir número de valoraciones por item_id
    conteo_valoraciones = data.groupby('item_id').size().reset_index(name='num_valoraciones')
    recomendaciones = recomendaciones.merge(conteo_valoraciones, on='item_id', how='left')

    # Ordenar y seleccionar top N
    top_recomendaciones = recomendaciones.sort_values(by='predicho', ascending=False).head(top_n)

    return top_recomendaciones[['titulo', 'predicho', 'num_valoraciones']]

#Generamos recomendaciones para usuario concreto
reco_usuario_nn = recomendar_peliculas_usuario(
    "123", data_train, modelo, user2idx, item2idx, idx2item, top_n=10
)
print(reco_usuario_nn) #Mostramos resultado

"""Vamos, como en los otros modelos a construir necesario para poder evaluar precision@k, recall@k y hit-rate@k para poder comparar modelos. Creamos funcion recomendar_para_usuario_evaluacion_nn. Esta función permite generar predicciones para un usuario individual, exclusivamente sobre las películas que no ha visto aún (según data_train), con el objetivo de evaluar el modelo."""

def recomendar_para_usuario_evaluacion_nn(user_id_original, top_n=10):
    if user_id_original not in user2idx:
        return []

    user_idx = user2idx[user_id_original]
    peliculas_vistas = set(data_train[data_train['user_id'] == user_id_original]['item_id'])
    peliculas_no_vistas = [iid for iid in item2idx if iid not in peliculas_vistas]

    item_idx_no_vistas = [item2idx[iid] for iid in peliculas_no_vistas if iid in item2idx]
    user_idx_array = [user_idx] * len(item_idx_no_vistas)

    if not item_idx_no_vistas:
        return []

    predicciones = modelo.predict([np.array(user_idx_array), np.array(item_idx_no_vistas)], verbose=0)
    predicciones = np.clip(predicciones, 1.0, 5.0)
    predicciones_ordenadas = sorted(
        zip(peliculas_no_vistas, predicciones.flatten()),
        key=lambda x: -x[1]
    )[:top_n]

    return [(user_id_original, item_id, score) for item_id, score in predicciones_ordenadas]

"""Aplicamos la función recomendar_para_usuario_evaluacion_nn a todos los usuarios del conjunto data_test."""

#------------------------------------------------------------------------------
# Ejecutamos evaluación top-K para todos los usuarios del test
#------------------------------------------------------------------------------

usuarios_test_nn = data_test['user_id'].unique()
predicciones_nn = []

for uid in usuarios_test_nn:
    predicciones_nn.extend(recomendar_para_usuario_evaluacion_nn(uid, top_n=10))

print(f"Número total de predicciones generadas (NN): {len(predicciones_nn)}")

"""Calculamos precision@k, recall@k y hitrate@k aprovechando funciones definidas anteriormente"""

precision_nn = precision_at_k_contenido(predicciones_nn, data_test, k=10)
recall_nn = recall_at_k_contenido(predicciones_nn, data_test, k=10)
hitrate_nn = hit_rate_at_k_contenido(predicciones_nn, data_test, k=10)

print(f"\nPrecision@10 (Neural Net): {precision_nn:.4f}")
print(f"Recall@10    (Neural Net): {recall_nn:.4f}")
print(f"Hit Rate@10  (Neural Net): {hitrate_nn:.4f}")

"""#Autoencoder

A diferencia del modelo con embeddings, el autoencoder trabaja con una matriz completa de valoraciones donde:
- Cada fila representa un usuario.
- Cada columna representa una película.
- Las celdas contienen las valoraciones (o cero si el usuario no valoró esa película).
"""

#------------------------------------------------------------------------------
#Autoencoder
#------------------------------------------------------------------------------

#Ordenamos los identificadores de usuarios e items
usuarios_ordenados = sorted(data_train['user_id'].unique())
peliculas_ordenadas = sorted(data_train['item_id'].unique())

# Creamos matriz de valoraciones (usuarios × ítems)
matriz = pd.DataFrame(0, index=usuarios_ordenados, columns=peliculas_ordenadas)

# Rellenamos con valoraciones reales (solo data_train)
for fila in data_train.itertuples():
    matriz.at[fila.user_id, fila.item_id] = fila.rating

# Visualizamos parte de la matriz
print(matriz.head())

# Convertimos a array para Keras
X_auto = matriz.values.astype('float32')

"""Creamos funcion de pérdida enmascarada. Esta funcion es clave para que el autoencoder solo aprenda a reconstruir las valoraciones conocidas (no los ceros que indican no visto)"""

# -----------------------------
# Función de pérdida enmascarada
# -----------------------------
import tensorflow as tf

def masked_mse(y_true, y_pred):
    mask = tf.cast(tf.not_equal(y_true, 0.0), tf.float32)
    squared_error = tf.square(y_true - y_pred)
    masked_error = squared_error * mask
    return tf.reduce_sum(masked_error) / tf.reduce_sum(mask)

"""Definimos el modelo autoencoder. Tomamos como entrada el vector de valoraciones de un usuario. Lo codificamos en un espacio de dimension menor (bottleneck). Luego intenamos reconstruir el vector completo como salida. Como dijimos antes,se utiliza la función de pérdida enmascarada (masked_mse) para que el modelo aprenda solo a partir de valoraciones existentes."""

# ----------------------
# Definición del modelo
# ----------------------
input_layer = Input(shape=(X_auto.shape[1],), name='entrada_usuario')

# Codificador
encoded = Dense(128, activation='relu')(input_layer)
encoded = Dense(64, activation='relu')(encoded)

# Bottleneck
bottleneck = Dense(32, activation='relu')(encoded)

# Decodificador
decoded = Dense(64, activation='relu')(bottleneck)
decoded = Dense(128, activation='relu')(decoded)
output_layer = Dense(X_auto.shape[1], activation='linear')(decoded)

# Modelo
autoencoder = Model(inputs=input_layer, outputs=output_layer)

# Compilación
autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=masked_mse)

# Resumen
autoencoder.summary()

early_stop_ae = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

"""Entrenamos el autoencoder"""

import time

# Entrenamiento con medición de tiempo
start_fit_ae = time.time()

hist_auto = autoencoder.fit(
    X_auto, X_auto,
    batch_size=64,
    epochs=20,
    validation_split=0.2,
    verbose=1,
    callbacks=[early_stop_ae]
)

end_fit_ae = time.time()
print(f" Tiempo de entrenamiento (autoencoder): {end_fit_ae - start_fit_ae:.2f} segundos")

"""Este bloque genera recomendaciones para un usuario específico utilizando el modelo autoencoder entrenado."""

# Usuario a recomendar
user_id = "123"

# Nos aseguramos de que esté en la matriz (solo usuarios de data_train)
if user_id in matriz.index:

    input_usuario = matriz.loc[user_id].values.reshape(1, -1)

    # Predicción con medición de tiempo
    start_pred_ae = time.time()

    predicciones = np.clip(autoencoder.predict(input_usuario).flatten(), 1.0, 5.0)

    end_pred_ae = time.time()
    print(f" Tiempo de predicción para usuario {user_id} (autoencoder): {end_pred_ae - start_pred_ae:.4f} segundos")

    # DataFrame con resultados
    df_reco = pd.DataFrame({
        'item_id': matriz.columns,
        'prediccion': predicciones,
        'rating_real': matriz.loc[user_id].values
    })

    # Filtrar películas no vistas
    pelis_no_vistas = df_reco[df_reco['rating_real'] == 0]

    # Añadir títulos
    pelis_no_vistas = pelis_no_vistas.merge(
        data[['item_id', 'titulo']].drop_duplicates(),
        on='item_id', how='left'
    )

    # Añadir número de valoraciones por película
    conteo_valoraciones = data.groupby('item_id').size().reset_index(name='num_valoraciones')
    pelis_no_vistas = pelis_no_vistas.merge(conteo_valoraciones, on='item_id', how='left')

    # Mostrar top 10 recomendaciones
    top_recomendaciones = pelis_no_vistas.sort_values(by='prediccion', ascending=False).head(10)

    print(f"\n Top 10 recomendaciones para el usuario {user_id} (autoencoder):\n")
    print(top_recomendaciones[['titulo', 'prediccion', 'num_valoraciones']])
else:
    print(f" Usuario {user_id} no está en el conjunto de entrenamiento.")

"""En este bloque evaluamos el rendimiento del autoencoder utilizando el conjunto data_test. Pasos realizados:
1. Se generan predicciones para cada usuario de test que también esté en data_train (usuarios conocidos).
2. Para cada usuario, se reconstruye su vector completo de valoraciones con el autoencoder.
3. Se almacena en caché ese vector para evitar predicciones duplicadas.
4. Luego, para cada fila de data_test, se recupera la predicción correspondiente al ítem evaluado.
5. Se calcula el RMSE final entre las valoraciones reales (y_real_ae) y las predichas (y_pred_ae).

"""

#Evaluamos modelo en data test

# Cacheamos todas las predicciones de usuarios conocidos
predicciones_por_usuario = {}

for uid in data_test['user_id'].unique():
    if uid in matriz.index:
        input_usuario = matriz.loc[uid].values.reshape(1, -1)
        pred_vector = autoencoder.predict(input_usuario, verbose=0).flatten()
        pred_vector = np.clip(pred_vector, 1.0, 5.0)
        predicciones_por_usuario[uid] = pred_vector

# Evaluamos usando ese caché
predicciones_ae = []
y_real_ae = []
y_pred_ae = []

start_global_pred_ae = time.time()

for fila in data_test.itertuples():
    uid = fila.user_id
    iid = fila.item_id
    rating_real = fila.rating

    if uid not in predicciones_por_usuario or iid not in matriz.columns:
        continue

    pred_vector = predicciones_por_usuario[uid]
    pred_rating = pred_vector[matriz.columns.get_loc(iid)]

    predicciones_ae.append((uid, iid, pred_rating))
    y_real_ae.append(rating_real)
    y_pred_ae.append(pred_rating)

end_global_pred_ae = time.time()

# Métricas finales
rmse_ae = np.sqrt(mean_squared_error(y_real_ae, y_pred_ae))
print(f" RMSE en test (autoencoder): {rmse_ae:.4f}")
print(f" Tiempo total de predicción en test (autoencoder): {end_global_pred_ae - start_global_pred_ae:.2f} segundos")

"""Generamos top 10 predicciones para cada usuario de data_test utiliznado el autoencoder."""

# Generar top-K recomendaciones por usuario (solo ítems no vistos)
predicciones_topk_ae = []

for uid in data_test['user_id'].unique():
    if uid not in predicciones_por_usuario:
        continue

    pred_vector = predicciones_por_usuario[uid]
    peliculas_vistas = set(data_train[data_train['user_id'] == uid]['item_id'])

    peliculas_no_vistas = [iid for iid in matriz.columns if iid not in peliculas_vistas]

    predicciones = [
        (uid, iid, pred_vector[matriz.columns.get_loc(iid)])
        for iid in peliculas_no_vistas
    ]

    top_k = sorted(predicciones, key=lambda x: -x[2])[:10]
    predicciones_topk_ae.extend(top_k)

"""Calculamos precision@k, recall@k y hitrate@k"""

#------------------------------------------------------------------------------
# Cálculo de métricas Precision@10, Recall@10, HitRate@10 (Autoencoder)
#------------------------------------------------------------------------------

precision_ae = precision_at_k_contenido(predicciones_topk_ae, data_test, k=10)
recall_ae = recall_at_k_contenido(predicciones_topk_ae, data_test, k=10)
hitrate_ae = hit_rate_at_k_contenido(predicciones_topk_ae, data_test, k=10)

print(f"\nPrecision@10 (Autoencoder): {precision_ae:.4f}")
print(f"Recall@10    (Autoencoder): {recall_ae:.4f}")
print(f"Hit Rate@10  (Autoencoder): {hitrate_ae:.4f}")

"""#Modelo deep learning híbrido

Como paso inicial del modelo híbrido, codificamos los identificadores reales (user_id, item_id) en índices enteros consecutivos necesarios para las capas Embedding.
"""

#------------------------------------------------------------------------------
# Modelo deep learning Hibrido
#------------------------------------------------------------------------------

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split


user2idx = {user_id: idx for idx, user_id in enumerate(data_train['user_id'].unique())}
item2idx = {item_id: idx for idx, item_id in enumerate(data_train['item_id'].unique())}
idx2item = {v: k for k, v in item2idx.items()}

data_train['user_idx'] = data_train['user_id'].map(user2idx)
data_train['item_idx'] = data_train['item_id'].map(item2idx)
data_test['user_idx'] = data_test['user_id'].map(user2idx)
data_test['item_idx'] = data_test['item_id'].map(item2idx)

# Eliminamos entradas de test con usuarios o ítems no vistos
data_test = data_test.dropna(subset=['user_idx', 'item_idx']).astype({'user_idx': int, 'item_idx': int})

"""Añadimos variables adicionales al modelo híbrido para enriquecer la representación de cada usuario. Edad normalizada ajustada con mix-max scaler. Codificamos ocupacion"""

# ------------------------------------------------------------------------------
# Normalización de edad (usando solo data_train para ajustar el scaler)
# ------------------------------------------------------------------------------
scaler = MinMaxScaler()
data_train['age_norm'] = scaler.fit_transform(data_train[['age']])
data_test['age_norm'] = scaler.transform(data_test[['age']])  # usamos el mismo scaler

# ------------------------------------------------------------------------------
# Codificación de ocupación como índice
# ------------------------------------------------------------------------------
ocupaciones = data_train['occupation'].unique()
ocup2idx = {ocup: i for i, ocup in enumerate(ocupaciones)}
data_train['occupation_idx'] = data_train['occupation'].map(ocup2idx)
data_test['occupation_idx'] = data_test['occupation'].map(ocup2idx)

# Eliminamos filas de test con ocupaciones no vistas en train
data_test = data_test.dropna(subset=['occupation_idx']).astype({'occupation_idx': int})

"""Para dotar al modelo híbrido de información basada en contenido, incorporamos los géneros de cada película como una entrada one-hot."""

# ------------------------------------------------------------------------------
# One-hot de géneros
# ------------------------------------------------------------------------------
columnas_generos = [
    'unknown', 'Action', 'Adventure', 'Animation', "Children's", 'Comedy',
    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'
]

# Extraemos géneros por película
generos = data_train[['item_id'] + columnas_generos].drop_duplicates('item_id')
generos.set_index('item_id', inplace=True)

# Asignamos géneros a cada fila
data_train = data_train.join(generos, on='item_id', rsuffix='_genero')
data_test = data_test.join(generos, on='item_id', rsuffix='_genero')

# Eliminamos filas de test con ítems sin géneros
data_test = data_test.dropna(subset=columnas_generos)

"""Este modelo híbrido combina información colaborativa (IDs codificados con embeddings) con variables de contenido (edad, ocupación, géneros) para predecir las valoraciones de los usuarios. Definimos componentes de entrada y la estructura de nuestro modelo"""

# Calculamos dimensiones
n_users = data_train['user_idx'].nunique()
n_items = data_train['item_idx'].nunique()
n_ocup = data_train['occupation_idx'].nunique()
n_generos = len(columnas_generos)
embedding_size = 50

# Entradas
input_user = Input(shape=(1,), name='user_input')
input_item = Input(shape=(1,), name='item_input')
input_age = Input(shape=(1,), name='age_input')
input_ocup = Input(shape=(1,), name='occupation_input')
input_generos = Input(shape=(n_generos,), name='generos_input')

# Embeddings (solo se calculan de variables categoricas)
embedding_user = Embedding(input_dim=n_users, output_dim=embedding_size)(input_user)
embedding_item = Embedding(input_dim=n_items, output_dim=embedding_size)(input_item)
embedding_ocup = Embedding(input_dim=n_ocup, output_dim=5)(input_ocup)

# Aplanar
user_vec = Flatten()(embedding_user)
item_vec = Flatten()(embedding_item)
ocup_vec = Flatten()(embedding_ocup)

# Concatenar todas las entradas
entrada_concatenada = Concatenate()([
    user_vec,
    item_vec,
    ocup_vec,
    input_age,
    input_generos
])

# Capas ocultas
x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(entrada_concatenada)
x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)
salida = Dense(1, activation='linear')(x)

# Modelo
modelo_hibrido = Model(
    inputs=[input_user, input_item, input_age, input_ocup, input_generos],
    outputs=salida
)

modelo_hibrido.compile(
    loss='mean_squared_error',
    optimizer=Adam(learning_rate=0.001),
    metrics=['RootMeanSquaredError']
)

# Mostrar resumen
modelo_hibrido.summary()

"""Realizamos entrenamiento del modelo híbrido"""

# 1) Prepara arrays directos de train y test
X_user_tr = data_train['user_idx'].values
X_item_tr = data_train['item_idx'].values
X_age_tr  = data_train['age_norm'].values
X_ocup_tr = data_train['occupation_idx'].values
X_gen_tr  = data_train[columnas_generos].values
y_tr      = data_train['rating'].values

X_user_te = data_test['user_idx'].values
X_item_te = data_test['item_idx'].values
X_age_te  = data_test['age_norm'].values
X_ocup_te = data_test['occupation_idx'].values
X_gen_te  = data_test[columnas_generos].values
y_te      = data_test['rating'].values

early_stopping = EarlyStopping(
    monitor='val_loss',    # observa la pérdida en validación
    patience=3,            # si no mejora en 3 épocas, se detiene
    restore_best_weights=True
)

# 2) Entrenamiento sobre TODO data_train
start_fit_hibrido = time.time()

historial_hibrido = modelo_hibrido.fit(
    [X_user_tr, X_item_tr, X_age_tr, X_ocup_tr, X_gen_tr],
    y_tr,
    validation_split=0.2,
    batch_size=64,
    epochs=30,                     # permitimos más épocas si es útil
    verbose=1,
    callbacks=[early_stopping]     # añadimos EarlySto
)

end_fit_hibrido = time.time()
print(f"Tiempo de entrenamiento (modelo híbrido): {end_fit_hibrido - start_fit_hibrido:.2f} segundos")

# 3) Predicción sobre TODO data_test
start_pred_hibrido = time.time()
y_pred_hibrido = modelo_hibrido.predict(
    [X_user_te, X_item_te, X_age_te, X_ocup_te, X_gen_te],
    verbose=0
)
end_pred_hibrido = time.time()

# 4) Clip al rango [1,5] y RMSE sobre data_test
y_pred_hibrido = np.clip(y_pred_hibrido, 1.0, 5.0)
rmse_hibrido = np.sqrt(mean_squared_error(y_te, y_pred_hibrido))
print(f"RMSE en test (modelo híbrido): {rmse_hibrido:.4f}")
print(f"Tiempo de predicción en test (modelo híbrido): {end_pred_hibrido - start_pred_hibrido:.2f} segundos")

"""Evaluamos RMSE"""

# 3) Predicción sobre TODO data_test
start_pred_hibrido = time.time()
y_pred_hibrido = modelo_hibrido.predict(
    [X_user_te, X_item_te, X_age_te, X_ocup_te, X_gen_te],
    verbose=0
)
end_pred_hibrido = time.time()

# 4) Clip al rango [1,5] y RMSE sobre data_test
y_pred_hibrido = np.clip(y_pred_hibrido, 1.0, 5.0)
rmse_hibrido = np.sqrt(mean_squared_error(y_te, y_pred_hibrido))
print(f"RMSE en test (modelo híbrido): {rmse_hibrido:.4f}")
print(f"Tiempo de predicción en test (modelo híbrido): {end_pred_hibrido - start_pred_hibrido:.2f} segundos")

"""Mejores y peores predichas"""

# Creamos DataFrame con resultados usando los arrays de test
df_eval_hibrido = pd.DataFrame({
    'user_idx':    X_user_te,
    'item_idx':    X_item_te,
    'rating_real': y_te,
    'rating_pred': y_pred_hibrido.flatten()
})


# Revertimos índices → item_id y user_id reales
idx2item = {v: k for k, v in item2idx.items()}
idx2user = {v: k for k, v in user2idx.items()}
df_eval_hibrido['item_id'] = df_eval_hibrido['item_idx'].map(idx2item)
df_eval_hibrido['user_id'] = df_eval_hibrido['user_idx'].map(idx2user)

#  Añadimos títulos usando data completo
df_eval_hibrido = df_eval_hibrido.merge(
    data[['item_id', 'titulo']].drop_duplicates(),
    on='item_id',
    how='left'
)

# Agrupamos por película
df_stats_hibrido = df_eval_hibrido.groupby('item_id').agg(
    num_valoraciones=('rating_real', 'count'),
    media_real=('rating_real', 'mean'),
    media_predicha=('rating_pred', 'mean'),
    titulo=('titulo', 'first')
).reset_index()

# Calculamos diferencia
df_stats_hibrido['diferencia'] = abs(df_stats_hibrido['media_real'] - df_stats_hibrido['media_predicha'])

# Top 10 mejor predichas
mejor_predichas_hibrido = df_stats_hibrido.sort_values(by='diferencia').head(10)

# Top 10 peor predichas
peor_predichas_hibrido = df_stats_hibrido.sort_values(by='diferencia', ascending=False).head(10)

# Mostramos resultados
print("\n Películas mejor predichas (modelo híbrido):\n")
print(mejor_predichas_hibrido[['titulo', 'media_real', 'media_predicha', 'diferencia', 'num_valoraciones']])

print("\n Películas peor predichas (modelo híbrido):\n")
print(peor_predichas_hibrido[['titulo', 'media_real', 'media_predicha', 'diferencia', 'num_valoraciones']])

"""Representamos como resto de modelos"""

# Boxplot de predicciones del modelo híbrido
df_resultados_hibrido = pd.DataFrame({
    'real': y_test,
    'predicha': y_pred_hibrido.flatten()
})

plt.figure(figsize=(8, 6))
sns.boxplot(x='real', y='predicha', data=df_resultados_hibrido)
plt.title("Distribución de valoraciones predichas según la real (Modelo Híbrido)")
plt.xlabel("Valoración real")
plt.ylabel("Valoración predicha")
plt.grid(True)
plt.show()

# Cálculo del error absoluto y resumen estadístico
df_resultados_hibrido['error'] = abs(df_resultados_hibrido['real'] - df_resultados_hibrido['predicha'])
print("\nResumen de errores (Modelo Híbrido):")
print(df_resultados_hibrido['error'].describe())

"""Recomendacion para usuario concreto"""

#Recomendación para usuario concreto

# Recomendaciones personalizadas para un usuario
user_id = "123"

if user_id not in user2idx:
    print(" Usuario no conocido en entrenamiento.")
else:
    user_idx = user2idx[user_id]

    # Películas ya vistas (usamos data_train como referencia válida)
    pelis_vistas = data_train[data_train['user_id'] == user_id]['item_id'].unique()

    # Películas no vistas
    pelis_no_vistas = data_train[~data_train['item_id'].isin(pelis_vistas)][['item_id', 'titulo']].drop_duplicates()

    # Preparamos entradas para predecir
    X_user_input = np.array([user_idx] * len(pelis_no_vistas))
    X_item_input = pelis_no_vistas['item_id'].map(item2idx).values
    X_age_input = np.array([data_train.loc[data_train['user_id'] == user_id, 'age_norm'].iloc[0]] * len(pelis_no_vistas))
    X_ocup_input = np.array([data_train.loc[data_train['user_id'] == user_id, 'occupation_idx'].iloc[0]] * len(pelis_no_vistas))
    X_generos_input = data_train.drop_duplicates('item_id').set_index('item_id').loc[
        pelis_no_vistas['item_id']
    ][columnas_generos].values

    # Predicciones
    predicciones_usuario = modelo_hibrido.predict([
        X_user_input,
        X_item_input,
        X_age_input,
        X_ocup_input,
        X_generos_input
    ], verbose=0)

    predicciones_usuario = np.clip(predicciones_usuario, 1.0, 5.0)
    pelis_no_vistas = pelis_no_vistas.copy()
    pelis_no_vistas['prediccion'] = predicciones_usuario.flatten()

    # Top 10

    top10_recomendaciones = pelis_no_vistas.sort_values(by='prediccion', ascending=False).head(10)

# Añadimos columna con el número de valoraciones de cada película en el conjunto de entrenamiento
    conteo_valoraciones = data_train.groupby('item_id')['rating'].count().rename("num_valoraciones")
    top10_recomendaciones = top10_recomendaciones.merge(conteo_valoraciones, on='item_id', how='left')

# Mostramos resultado con título, predicción y número de valoraciones
    print(f"\n Top 10 recomendaciones para el usuario {user_id} (modelo híbrido):\n")
    print(top10_recomendaciones[['titulo', 'prediccion', 'num_valoraciones']])

"""Generamos predicciones topk por usuario para obtener precision@k, recall@k y hit-rate@k (TARDA UNOS 5 minutos en ejecutar)"""

# Generamos predicciones top-K por usuario
predicciones_topk_hibrido = []

for uid in data_test['user_id'].unique():
    if uid not in user2idx:
        continue

    user_idx = user2idx[uid]
    pelis_vistas = set(data_train[data_train['user_id'] == uid]['item_id'])

    pelis_no_vistas = data_train[~data_train['item_id'].isin(pelis_vistas)][['item_id']].drop_duplicates()
    pelis_no_vistas = pelis_no_vistas[pelis_no_vistas['item_id'].isin(item2idx)]

    if pelis_no_vistas.empty:
        continue

    # Datos del usuario
    age = data_train.loc[data_train['user_id'] == uid, 'age_norm'].iloc[0]
    ocup = data_train.loc[data_train['user_id'] == uid, 'occupation_idx'].iloc[0]

    # Generamos inputs para predicción
    X_user_input = np.array([user_idx] * len(pelis_no_vistas))
    X_item_input = pelis_no_vistas['item_id'].map(item2idx).values
    X_age_input = np.array([age] * len(pelis_no_vistas))
    X_ocup_input = np.array([ocup] * len(pelis_no_vistas))

    X_generos_input = data_train.drop_duplicates('item_id').set_index('item_id').loc[
        pelis_no_vistas['item_id']
    ][columnas_generos].values

    predicciones = modelo_hibrido.predict([
        X_user_input,
        X_item_input,
        X_age_input,
        X_ocup_input,
        X_generos_input
    ], verbose=0)

    pelis_no_vistas = pelis_no_vistas.copy()
    pelis_no_vistas['score'] = predicciones.flatten()
    pelis_no_vistas['user_id'] = uid

    top_k = pelis_no_vistas.sort_values(by='score', ascending=False).head(10)

    predicciones_topk_hibrido.extend(list(zip(
        top_k['user_id'], top_k['item_id'], top_k['score']
    )))

"""Calculamos métricas"""

#------------------------------------------------------------------------------
# Cálculo de métricas Precision@10, Recall@10, HitRate@10 (modelo híbrido)
#------------------------------------------------------------------------------

precision_hibrido = precision_at_k_contenido(predicciones_topk_hibrido, data_test, k=10)
recall_hibrido = recall_at_k_contenido(predicciones_topk_hibrido, data_test, k=10)
hitrate_hibrido = hit_rate_at_k_contenido(predicciones_topk_hibrido, data_test, k=10)

print(f"\nPrecision@10 (Híbrido): {precision_hibrido:.4f}")
print(f"Recall@10    (Híbrido): {recall_hibrido:.4f}")
print(f"Hit Rate@10  (Híbrido): {hitrate_hibrido:.4f}")